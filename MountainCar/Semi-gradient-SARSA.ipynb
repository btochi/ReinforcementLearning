{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')\n",
    "from gym import wrappers\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "env = wrappers.Monitor(env, './videos/' + str(time()) + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6 , 0.07], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2 , -0.07], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we test a simple agent?\n",
    "\n",
    "Say our agent accelerate forwards always. What do you think would happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env,agent,number_episodes,steps_per_episode,render=False):\n",
    "    rewards = [0 for i in range(number_episodes)]\n",
    "    for i_episode in range(number_episodes):\n",
    "        observation = env.reset()\n",
    "        for t in range(steps_per_episode):\n",
    "            if render:\n",
    "                env.render()\n",
    "            #print(observation)\n",
    "            observation, reward, done, info = env.step(agent(observation)) #iorio\n",
    "            rewards[i_episode] += reward\n",
    "            if done:\n",
    "                #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "        if render:\n",
    "            env.close()\n",
    "    return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_forward(state):\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-200.0, -200.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agent(env,go_forward,2,200,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will produce \"linear agents\" i.e., for each possible action \"a\" in the action space,\n",
    "we have vector w_a sucht that  \n",
    "\n",
    "q(s,a) = w_a * x(s)  \n",
    "\n",
    "where x is a feature function. The w_a's assemble in a matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_agent(W,feature_func):\n",
    "    def agent(state):\n",
    "        features = feature_func(state)\n",
    "        #print(features.shape)\n",
    "        action = np.matmul(W,features).argmax()\n",
    "        return action\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we train our agent?  \n",
    "\n",
    "We follow semi-gradient SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_linear_SARSA(env,set_random,feature_func,d,alpha,epsilon,episodes,gamma,max_steps):\n",
    "    \n",
    "    #initialize weight vectors\n",
    "    \n",
    "    \n",
    "    W = np.zeros((env.action_space.n,d))\n",
    "    \n",
    "    best = 500\n",
    "    saved = None\n",
    "    scores = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "#uncomment this to log how our agent is doing during training.\n",
    "#         if episode%500 == 0:\n",
    "#             rewards = test_agent(env,linear_agent(W,feature_func),200,200,render=False)\n",
    "#             score = -sum(rewards)/len(rewards)\n",
    "#             #print(score,\"at\",episode,\"episode\")\n",
    "#             scores.append((episode,score))\n",
    "#             if score < best:\n",
    "#                 best = score\n",
    "#                 saved = linear_agent(W,feature_func)\n",
    "#                 #print(\"best agent so far\")\n",
    "        \n",
    "        #we reset the environment\n",
    "        set_random(env)\n",
    "        \n",
    "        \n",
    "        #we transform the state into a feature state\n",
    "        state1 = feature_func(env.state)\n",
    "        \n",
    "        #we pick an initial action, greedily\n",
    "        action = greedy_pick(env,W,state1,epsilon)\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < max_steps:\n",
    "            \n",
    "            #perform the action, get new state and reward.\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            step+=1\n",
    "            state2 = feature_func(state2)\n",
    "\n",
    "            if done:\n",
    "                W[action,:] = W[action,:] + alpha*(reward-np.dot(W[action,:],state1))*state1\n",
    "                break\n",
    "            else:\n",
    "                action2 =  greedy_pick(env,W,state2,epsilon)\n",
    "                W[action,:] = (W[action,:] + \n",
    "                               alpha*(reward+gamma*np.dot(W[action2,:],state2)\n",
    "                                     -np.dot(W[action,:],state1))*state1)\n",
    "                state1 = state2\n",
    "                action = action2\n",
    "    print(\"best score is\",best)\n",
    "    print(\"\\n\")\n",
    "    return W,saved,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_pick(env,W,features,epsilon):\n",
    "    if np.random.rand(1)[0] < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.matmul(W,features).argmax()  \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for our particular env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_Mountain_Car(env):\n",
    "    env.reset()\n",
    "    env.state[0] = np.random.uniform(-1.2,0.6)\n",
    "    env.state[1] = np.random.uniform(-0.07,0.07)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to implement state aggregation.\n",
    "\n",
    "In a future version we will do tile coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def find_interval(partition,observation):\n",
    "    return bisect_left(partition,observation)-1\n",
    "\n",
    "def state_aggregation(env,N):\n",
    "    \n",
    "    #N represents the number of points in the partition (so there are N-1 boxes)\n",
    "    \n",
    "    high = env.observation_space.high\n",
    "    low = env.observation_space.low\n",
    "    mesh = np.linspace(low,high,N)\n",
    "    D = high.shape[0]\n",
    "    \n",
    "    def feature(state):\n",
    "        coordinates = [0]*len(state)\n",
    "        for i in range(len(state)):\n",
    "            coordinates[i] = find_interval(mesh[:,i],state[i]) #not vectorized yet\n",
    "        answer = np.zeros((N-1,)*D)\n",
    "        answer[coordinates[0],coordinates[1]]=1 #horrible\n",
    "        return answer.reshape(-1)\n",
    "    \n",
    "    return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking hyperparameters is hard, so we'll do a small grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really solved this environment. We need to at least get to 110. Maybe with tile coding and n-step SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meshes(high,low,N,k):\n",
    "    size = (high-low)/(N-1)\n",
    "    return  [np.linspace(low-(i/k)*size,high+((k-i)/k)*size,N+1) for i in range(1,k)]+[np.linspace(low,high,N)]\n",
    "\n",
    "\n",
    "def tile_coding(env,N,k):\n",
    "    \n",
    "    high = env.observation_space.high\n",
    "    low = env.observation_space.low\n",
    "    \n",
    "    meshes = create_meshes(high,low,N,k)\n",
    "    D = high.shape[0]\n",
    "    \n",
    "    def feature(state):\n",
    "        answers = []\n",
    "        for mesh in meshes:\n",
    "            coordinates = [0]*len(state)\n",
    "            for i in range(len(state)):\n",
    "                coordinates[i] = find_interval(mesh[:,i],state[i]) #not vectorized yet\n",
    "            answer = np.zeros((N,)*D)\n",
    "            answer[coordinates[0],coordinates[1]]=1 #horrible\n",
    "            answers += [answer.reshape(-1)]\n",
    "    \n",
    "        return np.concatenate(answers)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_linear_n_step_SARSA(env,set_random,feature_func,n,d,alpha,epsilon,episodes,gamma,max_steps):\n",
    "    \n",
    "    #initialize weight vectors\n",
    "    \n",
    "    \n",
    "    W = np.zeros((env.action_space.n,d))\n",
    "    \n",
    "    best = inf\n",
    "    saved = None\n",
    "    saved_W = []\n",
    "    scores = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        if episode%500 == 0:\n",
    "            rewards = test_agent(env,linear_agent(W,feature_func),200,200,render=False)\n",
    "            score = -sum(rewards)/len(rewards)\n",
    "            print(score,\"at\",episode,\"episode\")\n",
    "            scores.append((episode,score))\n",
    "            if score < best:\n",
    "                best = score\n",
    "                saved = linear_agent(W,feature_func)\n",
    "                print(\"best agent so far\")\n",
    "        saved_W.append(W.copy())\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = [0]\n",
    "        \n",
    "        #we reset the environment\n",
    "        set_random(env)\n",
    "        \n",
    "        #we transform the state into a feature state\n",
    "        states.append(feature_func(env.state))\n",
    "        \n",
    "        #we pick an initial action, greedily\n",
    "        actions.append(greedy_pick(env,W,states[0],epsilon))\n",
    "        T = inf\n",
    "        tau = None\n",
    "        \n",
    "        t = 0\n",
    "        \n",
    "        while tau != T-1:\n",
    "            if t < T:\n",
    "                state, reward, done, info = env.step(actions[t])\n",
    "                states.append(feature_func(env.state))\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    T = t+1\n",
    "                else:\n",
    "                     actions.append(greedy_pick(env,W,states[-1],epsilon))\n",
    "\n",
    "            tau = t - n + 1 #time whose estimate is being updated\n",
    "            \n",
    "            if tau >= 0:\n",
    "                #we compute G\n",
    "                G = sum([(gamma**i)*rewards[i+tau+1] for i in range(0,min(tau+n,T)-tau)])\n",
    "                if tau + n < T:\n",
    "                    G += (gamma**n)*np.dot(W[actions[tau+n],:],states[tau+n])\n",
    "\n",
    "                #we update\n",
    "                W[actions[tau],:] = (W[actions[tau],:] + \n",
    "                               (alpha)*(G-np.dot(W[actions[tau],:],states[tau]))*states[tau])\n",
    "            t+=1\n",
    "    return W,saved,saved_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0 at 0 episode\n",
      "best agent so far\n",
      "187.24 at 500 episode\n",
      "best agent so far\n",
      "162.12 at 1000 episode\n",
      "best agent so far\n",
      "149.485 at 1500 episode\n",
      "best agent so far\n",
      "99.625 at 2000 episode\n",
      "best agent so far\n",
      "110.12 at 2500 episode\n",
      "121.05 at 3000 episode\n",
      "111.145 at 3500 episode\n",
      "105.505 at 4000 episode\n",
      "110.595 at 4500 episode\n",
      "105.555 at 5000 episode\n"
     ]
    }
   ],
   "source": [
    "N = 9\n",
    "k = 6\n",
    "n = 9\n",
    "alpha = 0.0325\n",
    "d = k*(N**2)\n",
    "\n",
    "from math import inf\n",
    "\n",
    "feature_func = tile_coding(env,N,k)\n",
    "\n",
    "result = semi_gradient_linear_n_step_SARSA(env,set_random_Mountain_Car,feature_func,\n",
    "                                           n,d,\n",
    "                                           alpha=alpha/k,\n",
    "                                           epsilon = 0,\n",
    "                                           episodes = 5001,\n",
    "                                           gamma = 1,\n",
    "                                           max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
